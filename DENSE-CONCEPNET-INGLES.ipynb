{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías\n",
    "import numpy as np\n",
    "import math as mt\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#import gensim\n",
    "#####NUEVAS######\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import MaxPooling1D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling2D\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hunspell import Hunspell\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(path):\n",
    "    #Sección de Variables\n",
    "    col_names = [\"ID\",\"TEXTO\",\"POLARIDAD\"]\n",
    "    #Lectura del archivo csv Train\\n\",\n",
    "    df = pd.read_csv(path, header=None, names = col_names)\n",
    "    length = len(df)-1\n",
    "    #Se elimina la primera columna donde esta en el encabezado ID,CONTENT,POLARITY\n",
    "    df = df.drop([0])\n",
    "    \n",
    "    #Se separan las sentencias y la polaridad\n",
    "    x_df=df['TEXTO']\n",
    "    y_df=df['POLARIDAD']    \n",
    "    \n",
    "    #Se limpian las sentencias(X_DF) y se asignan a X_SENTENCES, Ademas se crea un array de los LABEL(POLARIDAD)\n",
    "    x_sentences= clean_text(x_df)\n",
    "    x_sentences=np.array(x_sentences)\n",
    "    y_label= y_df.to_numpy()\n",
    "    #Separa la data en entrenamiento y prueba la prueba por ahora son del 25% de los datos\n",
    "    sentences_train, sentences_test, label_train, label_test = train_test_split(x_sentences,y_label, test_size=0.25, random_state=1000)    \n",
    "    return sentences_train, sentences_test, label_train, label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentencias, remove_stopwords = False):\n",
    "    sentences_clean=[]\n",
    "    for text in sentencias:\n",
    "        # convierte la sentencia a minuscula\n",
    "        text = text.lower()\n",
    "        #Elimina Caracteres que NO se usaran\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'&amp;', '', text) \n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "        text = re.sub(r'\\¿|“|”|¡|«|»|—|<|^|{|}|>', ' ', text)\n",
    "        text = re.sub(r'[\\n]', '', text)\n",
    "        \n",
    "        #remueve stop words\n",
    "        if remove_stopwords:\n",
    "            text = text.split()\n",
    "            stops = set(stopwords.words(\"English\"))\n",
    "            text = [w for w in text if not w in stops]\n",
    "            text = \" \".join(text)\n",
    "        sentences_clean.append(text)\n",
    "    \n",
    "    sentences_clean=np.array(sentences_clean)\n",
    "    return sentences_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding():\n",
    "    embeddings_index = {}\n",
    "    with open('numberbatch.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            tagEn=\"/c/en/\"\n",
    "            tagMul=\"/c/mul/\"\n",
    "            if tagEn in word or tagMul in word:                \n",
    "                embedding = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = embedding  \n",
    "    return embeddings_index     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_vector(sentences_train,sentences_test,embeddings_index):\n",
    "   #*************PROCESOS PARA EL ENTRENAMIENTO*****************\n",
    "    #Se llama a la funcion que crear el vector con los embeddings\n",
    "    trainData=sentences_train #Leer Data\n",
    "    i=0\n",
    "    j=0\n",
    "    h = Hunspell()\n",
    "    #Vector que almacenara el vector tokenizado y en word embedding\n",
    "    vectorEmbeddingsTrain=[]\n",
    "    while(i<len(trainData)):\n",
    "        #El texto esta plano en el array\n",
    "        texto=trainData[i]\n",
    "        #La función split divide el texto por los espacios creando un array\n",
    "        palabras=texto.split()\n",
    "        arrayAux=[]\n",
    "        j=0\n",
    "        while(j<len(palabras)):\n",
    "            try:\n",
    "                arrayAux.append(embeddings_index['/c/en/'+palabras[j].strip()])\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    arrayAux.append(embeddings_index['/c/mul/'+palabras[j].strip()])\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        palabraux=palabras[j]\n",
    "                        word=h.suggest(palabraux)\n",
    "                        if(len(word)>0):\n",
    "                            arrayAux.append(embeddings_index['/c/en/'+word[0]])\n",
    "                    except (UnicodeError, KeyError):\n",
    "                        pass\n",
    "            j+=1\n",
    "        i+=1\n",
    "        arrayAux=np.array(arrayAux)\n",
    "        vectorEmbeddingsTrain.append(arrayAux)\n",
    "    trainSentencesEmbedding=[]\n",
    "    i=0\n",
    "    while(i<len(vectorEmbeddingsTrain)):\n",
    "        trainSentencesEmbedding.append(np.average(vectorEmbeddingsTrain[i],axis=0))\n",
    "        i+=1\n",
    "     #*************PROCESOS PARA EL TEST*****************\n",
    "    #Se llama a la funcion que crear el vector con los embeddings\n",
    "    testData=sentences_test #Leer Data\n",
    "    i=0\n",
    "    j=0\n",
    "    #Vector que almacenara el vector tokenizado y en word embedding\n",
    "    vectorEmbeddingsTest=[]\n",
    "    while(i<len(testData)):\n",
    "        #El texto esta plano en el array\n",
    "        textoTest=testData[i]\n",
    "        #La función split divide el texto por los espacios creando un array\n",
    "        palabrasTest=textoTest.split()\n",
    "        arrayAuxTest=[]\n",
    "\n",
    "        j=0\n",
    "        while(j<len(palabrasTest)):\n",
    "            try:\n",
    "                arrayAuxTest.append(embeddings_index['/c/en/'+palabrasTest[j].strip()])\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    arrayAuxTest.append(embeddings_index['/c/mul/'+palabrasTest[j].strip()])\n",
    "                except KeyError:\n",
    "                        try:\n",
    "                            palabraux=palabrasTest[j]\n",
    "                            word=h.suggest(palabraux)\n",
    "                            if(len(word)>0):\n",
    "                                arrayAuxTest.append(embeddings_index['/c/en/'+word[0]])\n",
    "                        except (UnicodeError, KeyError):\n",
    "                            pass\n",
    "            j+=1\n",
    "        i+=1\n",
    "        arrayAuxTest=np.array(arrayAuxTest)\n",
    "        vectorEmbeddingsTest.append(arrayAuxTest)\n",
    "    testSentencesEmbedding=[]\n",
    "    i=0\n",
    "    while(i<len(vectorEmbeddingsTest)):\n",
    "        testSentencesEmbedding.append(np.average(vectorEmbeddingsTest[i],axis=0))\n",
    "        i+=1\n",
    "    trainSentencesEmbedding=np.array(trainSentencesEmbedding)\n",
    "    testSentencesEmbedding=np.array(testSentencesEmbedding)\n",
    "    return trainSentencesEmbedding,testSentencesEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaVector(vectorX, vectorY):\n",
    "    i=0\n",
    "    vectorXAux=[]\n",
    "    vectorYAux=[]\n",
    "    while(i<len(vectorX)):\n",
    "        if(vectorX[i].size==300):\n",
    "            vectorXAux.append(vectorX[i])\n",
    "            vectorYAux.append(vectorY[i])\n",
    "        i+=1\n",
    "    vectorXAux=np.array(vectorXAux)\n",
    "    vectorYAux=np.array(vectorYAux)\n",
    "    return vectorXAux,vectorYAux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se prepara la data\n",
    "sentences_train, sentences_test, label_train, label_test=processData(\"Data3polaridadSD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index=create_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se convierten los comentarios en un vector de tamaño 300\n",
    "sentences_Embeddins_Train,sentences_Embeddins_Test=sentences_to_vector(sentences_train,sentences_test,embeddings_index)\n",
    "#Se eliminan los campos vacios, para evitar cualquier error con la red\n",
    "sentences_Embeddins_Train,label_train=limpiaVector(sentences_Embeddins_Train,label_train)\n",
    "sentences_Embeddins_Test,label_test=limpiaVector(sentences_Embeddins_Test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se transforma las polaridades a un vector binario\n",
    "label_train=to_categorical(label_train,3)\n",
    "label_test=to_categorical(label_test,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones para calculos de recall, precision y f1\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se establecen las capas de la red\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(32, activation='relu',input_shape=(300,)))\n",
    "model.add(layers.Dense(32, activation='sigmoid'))\n",
    "model.add(layers.Dense(3, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy',f1_m,precision_m, recall_m])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicio del entrenamiento\n",
    "history = model.fit(sentences_Embeddins_Train, label_train,\n",
    "                    epochs=50,\n",
    "                    verbose=False,\n",
    "                    validation_data=(sentences_Embeddins_Test, label_test),\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, f1_score, precision, recall = model.evaluate(sentences_Embeddins_Train, label_train, verbose=0)\n",
    "print(\"Training results\")\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Training Loss: {:.4f}\".format(loss))\n",
    "print(\"Training f1_score: {:.4f}\".format(f1_score))\n",
    "print(\"Training Precisition: {:.4f}\".format(precision))\n",
    "print(\"Training Recall: {:.4f}\".format(recall))\n",
    "print(\"\\n\")\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(sentences_Embeddins_Test, label_test, verbose=0)\n",
    "print(\"Test results\")\n",
    "print(\"Test Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Test Loss: {:.4f}\".format(loss))\n",
    "print(\"Test f1_score: {:.4f}\".format(f1_score))\n",
    "print(\"Test Precisition: {:.4f}\".format(precision))\n",
    "print(\"Test Recall: {:.4f}\".format(recall))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones=model.predict(sentences_Embeddins_Test)\n",
    "prediction = pd.DataFrame(predicciones, columns=['predicciones','','']).to_csv('prediction.csv')\n",
    "cm = confusion_matrix(label_test.argmax(axis=1), predicciones.argmax(axis=1))\n",
    "labels = ['positivo', 'neutro', 'negativo']\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Matriz de confusión')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predecidos')\n",
    "plt.ylabel('Verdaderos')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
